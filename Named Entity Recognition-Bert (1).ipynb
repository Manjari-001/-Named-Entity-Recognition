{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffa134a6",
   "metadata": {},
   "source": [
    "# Data Loading and Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7236575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file containing sentences and tags\n",
    "dataset_path = \"ner_dataset.csv\"\n",
    "ner_df = pd.read_csv(dataset_path, encoding=\"latin1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14b5c222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>NaN</td>\n",
       "      <td>they</td>\n",
       "      <td>PRP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>NaN</td>\n",
       "      <td>responded</td>\n",
       "      <td>VBD</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>attack</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Sentence #           Word  POS Tag\n",
       "0        Sentence: 1      Thousands  NNS   O\n",
       "1                NaN             of   IN   O\n",
       "2                NaN  demonstrators  NNS   O\n",
       "3                NaN           have  VBP   O\n",
       "4                NaN        marched  VBN   O\n",
       "...              ...            ...  ...  ..\n",
       "1048570          NaN           they  PRP   O\n",
       "1048571          NaN      responded  VBD   O\n",
       "1048572          NaN             to   TO   O\n",
       "1048573          NaN            the   DT   O\n",
       "1048574          NaN         attack   NN   O\n",
       "\n",
       "[1048575 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ced920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf1ba284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    "train_df, test_df = train_test_split(ner_df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559143a",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7daf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data\n",
    "def preprocess_data(df):\n",
    "    sentences = []\n",
    "    ner_tags = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if the Sentence # column is not NaN\n",
    "        if not pd.isnull(row[\"Sentence #\"]):\n",
    "            # If it's not a blank sentence, add the current sentence and its tags to the lists\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                ner_tags.append(current_tags)\n",
    "            # Reset current sentence and tags for the new sentence\n",
    "            current_sentence = []\n",
    "            current_tags = []\n",
    "        # Add word and tag to the current sentence and tags\n",
    "        current_sentence.append(row[\"Word\"])\n",
    "        current_tags.append(row[\"Tag\"])\n",
    "    \n",
    "    # Append the last sentence and tags\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        ner_tags.append(current_tags)\n",
    "    \n",
    "    return sentences, ner_tags\n",
    "\n",
    "# Preprocess training, validation, and test data\n",
    "train_sentences, train_ner_tags = preprocess_data(train_df)\n",
    "val_sentences, val_ner_tags = preprocess_data(val_df)\n",
    "test_sentences, test_ner_tags = preprocess_data(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622bd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=ner_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb34f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both Conditional Random Fields (CRF) and \n",
    "#Bidirectional LSTM with a CRF layer (BiLSTM-CRF) are popular choices for Named Entity Recognition (NER) tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3410ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have limited training data or prefer an interpretable model with explicit modeling of sequential dependencies, CRF could be a suitable choice.\n",
    "# If you have abundant training data and prioritize performance over interpretability, BiLSTM-CRF may offer \n",
    "# better results by capturing richer contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ce4c4",
   "metadata": {},
   "source": [
    "# Feature Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8839a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(df):\n",
    "    sentences = []\n",
    "    ner_tags = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Check if the Sentence # column is not NaN\n",
    "        if not pd.isnull(row[\"Sentence #\"]):\n",
    "            # If it's not a blank sentence, add the current sentence and its tags to the lists\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                ner_tags.append(current_tags)\n",
    "            # Reset current sentence and tags for the new sentence\n",
    "            current_sentence = []\n",
    "            current_tags = []\n",
    "        # Add word and tag to the current sentence and tags\n",
    "        current_sentence.append(row[\"Word\"])\n",
    "        current_tags.append(row[\"Tag\"])\n",
    "    \n",
    "    # Append the last sentence and tags\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        ner_tags.append(current_tags)\n",
    "    \n",
    "    return sentences, ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "364e177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(sentence, index):\n",
    "    word = sentence[index]\n",
    "\n",
    "    # Define features for the word\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "    }\n",
    "\n",
    "    if index > 0:\n",
    "        prev_word = sentence[index - 1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': prev_word.lower(),\n",
    "            '-1:word.istitle()': prev_word.istitle(),\n",
    "            '-1:word.isupper()': prev_word.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True  # Beginning of sentence\n",
    "    \n",
    "    if index < len(sentence) - 1:\n",
    "        next_word = sentence[index + 1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': next_word.lower(),\n",
    "            '+1:word.istitle()': next_word.istitle(),\n",
    "            '+1:word.isupper()': next_word.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True  # End of sentence\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8040b8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from a sentence\n",
    "def sentence_features(sentence):\n",
    "    return [word_features(sentence, index) for index in range(len(sentence))]\n",
    "\n",
    "# Function to define labels for a sentence\n",
    "def sentence_labels(sentence):\n",
    "    return [label for label in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e226c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature extraction function call\n",
    "# Extract features and labels for the training data\n",
    "X_train = [sentence_features(sentence) for sentence in train_sentences]\n",
    "y_train = train_ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c743c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels for the validation data\n",
    "X_val = [sentence_features(sentence) for sentence in val_sentences]\n",
    "y_val = val_ner_tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c0d70d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and labels for the test data\n",
    "X_test = [sentence_features(sentence) for sentence in test_sentences]\n",
    "y_test = test_ner_tags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d2de5f",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d1ed9",
   "metadata": {},
   "source": [
    "# Bidirectional LSTM with a CRF layer (BiLSTM-CRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e3c0364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow keras keras-contrib numpy pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac45687",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Tokenization\n",
    "word_tokenizer = Tokenizer()  # Initialize tokenizer\n",
    "word_tokenizer.fit_on_texts(train_sentences)  # Fit tokenizer on training sentences\n",
    "\n",
    "# Convert words to sequences of integers\n",
    "X_train_seq = word_tokenizer.texts_to_sequences(train_sentences)\n",
    "X_val_seq = word_tokenizer.texts_to_sequences(val_sentences)\n",
    "X_test_seq = word_tokenizer.texts_to_sequences(test_sentences)\n",
    "\n",
    "# Padding\n",
    "max_seq_length = 100  # Define maximum sequence length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_seq_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Label Encoding\n",
    "label_encoder = LabelEncoder()  # Initialize label encoder\n",
    "label_encoder.fit([tag for tag_seq in train_ner_tags for tag in tag_seq])  # Fit label encoder on training tags\n",
    "\n",
    "# Encode NER tags\n",
    "y_train_encoded = [label_encoder.transform(tag_seq) for tag_seq in train_ner_tags]\n",
    "y_val_encoded = [label_encoder.transform(tag_seq) for tag_seq in val_ner_tags]\n",
    "y_test_encoded = [label_encoder.transform(tag_seq) for tag_seq in test_ner_tags]\n",
    "\n",
    "# Train-Validation Split\n",
    "# Already split during preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9385c2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjarilahariya/anaconda3/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,772,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">160,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,417</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m2,772,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m200\u001b[0m)       │       \u001b[38;5;34m160,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m17\u001b[0m)        │         \u001b[38;5;34m3,417\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,936,817</span> (11.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,936,817\u001b[0m (11.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,936,817</span> (11.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,936,817\u001b[0m (11.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Bidirectional, TimeDistributed, Dense, Dropout, Masking, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define the input shape\n",
    "input_layer = Input(shape=(max_seq_length,))\n",
    "\n",
    "# Add an embedding layer to convert input words into dense vectors\n",
    "embedding_layer = Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=100, input_length=max_seq_length)(input_layer)\n",
    "\n",
    "# Add a Bidirectional LSTM layer to capture bidirectional context\n",
    "lstm_layer = Bidirectional(LSTM(units=100, return_sequences=True, dropout=0.5))(embedding_layer)\n",
    "\n",
    "# Add a TimeDistributed layer to apply a dense layer to each time step\n",
    "dense_layer = TimeDistributed(Dense(units=len(label_encoder.classes_), activation=\"softmax\"))(lstm_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=dense_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fbb92b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 111ms/step - accuracy: 0.9521 - loss: 0.2153 - val_accuracy: 0.9729 - val_loss: 0.1735\n",
      "Epoch 2/5\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 127ms/step - accuracy: 0.9861 - loss: 0.0489 - val_accuracy: 0.9735 - val_loss: 0.1808\n",
      "Epoch 3/5\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 126ms/step - accuracy: 0.9878 - loss: 0.0415 - val_accuracy: 0.9737 - val_loss: 0.1825\n",
      "Epoch 4/5\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 121ms/step - accuracy: 0.9882 - loss: 0.0389 - val_accuracy: 0.9735 - val_loss: 0.1850\n",
      "Epoch 5/5\n",
      "\u001b[1m1076/1076\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 117ms/step - accuracy: 0.9884 - loss: 0.0375 - val_accuracy: 0.9734 - val_loss: 0.1896\n"
     ]
    }
   ],
   "source": [
    "# Train the BiLSTM-CRF Model\n",
    "\n",
    "# Convert NER tags to numpy arrays\n",
    "y_train_np = pad_sequences(y_train_encoded, maxlen=max_seq_length, padding='post')\n",
    "y_val_np = pad_sequences(y_val_encoded, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_pad, y_train_np, validation_data=(X_val_pad, y_val_np), epochs=5, batch_size=32, verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989739a",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f1fea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.9740 - loss: 0.1852\n",
      "Test Loss: 0.18620410561561584\n",
      "Test Accuracy: 0.9740674495697021\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test_pad, pad_sequences(y_test_encoded, maxlen=max_seq_length, padding='post'), verbose=1)\n",
    "\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e885267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m305/305\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 21ms/step\n",
      "Sentence: ['the', 'investors', 'New', 'his', 'the', 'Earth', 'modest', 'on', 'war', ',', 'officers', 'prompted', 'Sri', 'monthly', 'political', 'former', 'Palestinian', 'eliminate', '60', '.', 'Haiti', 'Jordan', 'under', 'Hezbollah', 'dialogue']\n",
      "True Labels: ['O', 'O', 'B-org', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'B-org', 'O', 'O', 'O', 'B-geo', 'B-gpe', 'O', 'B-org', 'O']\n",
      "Predicted Labels: ['O' 'O' 'O' 'O' 'O' 'B-geo' 'O' 'O' 'O' 'O' 'O' 'O' 'B-per' 'O' 'O' 'O'\n",
      " 'B-gpe' 'O' 'O' 'O' 'B-geo' 'B-gpe' 'O' 'B-org' 'O' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art']\n",
      "\n",
      "Sentence: ['Harry', 'was', 'assassinated', 'an', 'President', 'been', 'was', 'at', 'votes', 'rocket']\n",
      "True Labels: ['B-per', 'O', 'O', 'O', 'B-per', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted Labels: ['B-per' 'O' 'O' 'O' 'B-per' 'O' 'O' 'O' 'O' 'O' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art']\n",
      "\n",
      "Sentence: ['Chinese', 'economy', '.', 'soared', 'with', 'of', 'going', 'talks', 'condone', 'the']\n",
      "True Labels: ['B-gpe', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "Predicted Labels: ['B-gpe' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art']\n",
      "\n",
      "Sentence: ['In', 'responsible', 'Palestinian', ',', 'people', 'Hugo', 'the', 'retaking']\n",
      "True Labels: ['O', 'O', 'B-gpe', 'O', 'O', 'I-per', 'O', 'O']\n",
      "Predicted Labels: ['O' 'O' 'B-gpe' 'O' 'O' 'I-per' 'O' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art']\n",
      "\n",
      "Sentence: ['The', 'part', 'to', 'associated', 'say', 'close', 'could', 'New', 'shuttle', 'Almeida', 'have', 'an', 'Tehran']\n",
      "True Labels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-org', 'O', 'I-per', 'O', 'O', 'B-per']\n",
      "Predicted Labels: ['O' 'O' 'O' 'B-org' 'O' 'O' 'O' 'O' 'O' 'I-per' 'O' 'O' 'B-geo' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art' 'B-art'\n",
      " 'B-art' 'B-art' 'B-art' 'B-art' 'B-art']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Predict labels for the test data\n",
    "predictions = model.predict(X_test_pad)\n",
    "\n",
    "# Decode the predicted labels\n",
    "decoded_predictions = []\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    # Get the predicted label indices for the current sequence\n",
    "    predicted_indices = np.argmax(predictions[i], axis=1)\n",
    "    \n",
    "    # Decode the predicted label indices using the label encoder\n",
    "    predicted_labels = label_encoder.inverse_transform(predicted_indices)\n",
    "    \n",
    "    # Add the decoded predictions to the list\n",
    "    decoded_predictions.append(predicted_labels)\n",
    "\n",
    "# Print the first few decoded predictions\n",
    "for i in range(5):\n",
    "    print(\"Sentence:\", test_sentences[i])\n",
    "    print(\"True Labels:\",test_ner_tags[i])\n",
    "    print(\"Predicted Labels:\", decoded_predictions[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d1c1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #  We can adjust the model architecture or hyperparameters, increase the training data, or introduce \n",
    "# regularization techniques like dropout. For the Bert model, we can again check proper data preprocessing, \n",
    "# including tokenization and padding, and fine-tune the model on domain-specific data to enhance \n",
    "# performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65e69af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbce79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b7d2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e69a38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
